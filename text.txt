For weights in the hidden layer (w1, w2, w3, w4)
Similar calculations are made to update the weights in the hidden layer. However, this time the chain becomes a bit longer. It does not matter how deep the neural network goes, all we need to find out is how much error is propagated (contributed) by a particular weight to the total error of the network. For that purpose, we need to find the partial derivative of Error w.r.t. to the particular weight. Letâ€™s work on updating w1 and weâ€™ll be able to generalize similar calculations to update the rest of the weights.

For w1 (with respect to E1),

For simplicity let us compute 
âˆ‚
ğ¸
1
âˆ‚
ğ‘¤
1
âˆ‚w âˆ‚E 1 and 
âˆ‚
ğ¸
2
âˆ‚
ğ‘¤
1
âˆ‚w âˆ‚E 2 separately, and later we can add them to compute 
âˆ‚
ğ¸
ğ‘¡
ğ‘œ
ğ‘¡
ğ‘
ğ‘™
âˆ‚
ğ‘¤
1
âˆ‚w âˆ‚E total.

âˆ‚
ğ¸
1
âˆ‚
ğ‘¤
1
=
âˆ‚
ğ¸
1
âˆ‚
ğ‘œ
ğ‘¢
ğ‘¡
ğ‘
ğ‘¢
ğ‘¡
ğ‘œ
1
âˆ—
âˆ‚
ğ‘œ
ğ‘¢
ğ‘¡
ğ‘
ğ‘¢
ğ‘¡
ğ‘œ
1
âˆ‚
ğ‘ 
ğ‘¢
ğ‘š
ğ‘œ
1
âˆ—
âˆ‚
ğ‘ 
ğ‘¢
ğ‘š
ğ‘œ
1
âˆ‚
ğ‘œ
ğ‘¢
ğ‘¡
ğ‘
ğ‘¢
ğ‘¡
â„
1
âˆ—
âˆ‚
ğ‘œ
ğ‘¢
ğ‘¡
ğ‘
ğ‘¢
ğ‘¡
â„
1
âˆ‚
ğ‘ 
ğ‘¢
ğ‘š
â„
1
âˆ—
âˆ‚
ğ‘ 
ğ‘¢
ğ‘š
â„
1
âˆ‚
ğ‘¤
1
âˆ‚w âˆ‚E 1= 
âˆ‚output 
o1
â€‹
 âˆ‚E 1âˆ— 
âˆ‚sum 
o1
â€‹
 âˆ‚output o1âˆ— 
âˆ‚output 
h1
â€‹
 âˆ‚sum o1âˆ— 
âˆ‚sum 
h1
â€‹
 âˆ‚output h1âˆ— 
âˆ‚w1
âˆ‚sum 
h1
â€‹
 
â€‹
